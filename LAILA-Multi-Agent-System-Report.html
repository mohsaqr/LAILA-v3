<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>LAILA: Architecture of a Multi-Agent Tutoring System</title>
<style>
:root {
  --text: #1a1a2e;
  --text-secondary: #4a4a6a;
  --bg: #ffffff;
  --bg-alt: #f8f9fc;
  --accent: #2c3e7a;
  --accent-light: #e8ecf4;
  --border: #d0d5e0;
  --border-light: #e8ecf0;
  --code-bg: #f0f2f8;
  --diagram-node: #2c3e7a;
  --diagram-node-text: #ffffff;
  --diagram-edge: #8892b0;
  --diagram-node-alt: #5a6fa0;
  --diagram-node-highlight: #1a6b4a;
  --diagram-node-warn: #7a4a1a;
  --toc-hover: #eef0f8;
}

* { margin: 0; padding: 0; box-sizing: border-box; }

body {
  font-family: "Georgia", "Times New Roman", serif;
  font-size: 16px;
  line-height: 1.78;
  color: var(--text);
  background: var(--bg);
  max-width: 880px;
  margin: 0 auto;
  padding: 2rem 1.5rem 4rem;
}

h1, h2, h3, h4, h5 {
  font-family: "Helvetica Neue", "Arial", sans-serif;
  font-weight: 600;
  color: var(--accent);
  line-height: 1.3;
}

h1 { font-size: 2rem; margin-bottom: 0.5rem; }
h2 { font-size: 1.45rem; margin: 3rem 0 1.1rem; padding-bottom: 0.4rem; border-bottom: 2px solid var(--accent-light); }
h3 { font-size: 1.15rem; margin: 2rem 0 0.8rem; }
h4 { font-size: 1.02rem; margin: 1.5rem 0 0.6rem; }

p { margin-bottom: 1rem; text-align: justify; hyphens: auto; }

a { color: var(--accent); text-decoration: none; }
a:hover { text-decoration: underline; }

code {
  font-family: "SF Mono", "Fira Code", "Consolas", monospace;
  font-size: 0.86em;
  background: var(--code-bg);
  padding: 0.12em 0.35em;
  border-radius: 3px;
}

ul, ol { margin-bottom: 1rem; padding-left: 1.8rem; }
li { margin-bottom: 0.35rem; }

blockquote {
  margin: 1.2rem 0 1.5rem 0;
  padding: 0.9rem 1.2rem;
  background: var(--bg-alt);
  border-left: 4px solid var(--accent);
  font-size: 0.95rem;
  font-style: italic;
  border-radius: 0 6px 6px 0;
}

blockquote p { margin-bottom: 0.4rem; text-align: left; }
blockquote p:last-child { margin-bottom: 0; }

.title-page {
  text-align: center;
  padding: 4.5rem 0 3.5rem;
  border-bottom: 3px solid var(--accent);
  margin-bottom: 2.5rem;
}

.title-page h1 { font-size: 2.3rem; margin-bottom: 0.4rem; }
.title-page .subtitle { font-size: 1.1rem; color: var(--text-secondary); margin-bottom: 1.5rem; font-style: italic; }
.title-page .meta { font-size: 0.95rem; color: var(--text-secondary); }

/* TOC */
.toc {
  background: var(--bg-alt);
  border: 1px solid var(--border-light);
  border-radius: 8px;
  padding: 1.5rem 2rem;
  margin-bottom: 2.5rem;
}

.toc h2 { margin: 0 0 1rem; padding-bottom: 0; border-bottom: none; font-size: 1.15rem; }

.toc ol { list-style: none; padding-left: 0; counter-reset: toc; }
.toc > ol > li { counter-increment: toc; margin-bottom: 0.35rem; }
.toc > ol > li::before { content: counter(toc) "."; font-weight: 600; margin-right: 0.5rem; color: var(--accent); }
.toc > ol > li > ol { padding-left: 1.8rem; margin-top: 0.3rem; counter-reset: subtoc; }
.toc > ol > li > ol > li { counter-increment: subtoc; }
.toc > ol > li > ol > li::before { content: counter(toc) "." counter(subtoc); margin-right: 0.4rem; color: var(--text-secondary); font-size: 0.92em; }
.toc a { color: var(--text); }
.toc a:hover { color: var(--accent); background: var(--toc-hover); padding: 0.1em 0.3em; border-radius: 3px; }

/* Tables (only used sparingly) */
table {
  width: 100%;
  border-collapse: collapse;
  margin-bottom: 1.5rem;
  font-size: 0.92rem;
}
th, td { padding: 0.6rem 0.8rem; text-align: left; border: 1px solid var(--border); vertical-align: top; }
th { background: var(--accent); color: #fff; font-family: "Helvetica Neue", "Arial", sans-serif; font-weight: 600; font-size: 0.88rem; }
tr:nth-child(even) { background: var(--bg-alt); }

/* Diagrams */
.diagram {
  margin: 1.5rem 0 2rem;
  padding: 1.5rem;
  background: var(--bg-alt);
  border: 1px solid var(--border-light);
  border-radius: 8px;
}

.diagram-title {
  font-family: "Helvetica Neue", "Arial", sans-serif;
  font-size: 0.85rem;
  font-weight: 600;
  text-align: center;
  color: var(--text-secondary);
  margin-bottom: 1rem;
  text-transform: uppercase;
  letter-spacing: 0.04em;
}

.tier-diagram { display: flex; flex-direction: column; gap: 0.8rem; align-items: center; }
.tier { display: flex; gap: 0.6rem; justify-content: center; flex-wrap: wrap; width: 100%; }
.tier-label { font-family: "Helvetica Neue", "Arial", sans-serif; font-size: 0.78rem; text-transform: uppercase; letter-spacing: 0.06em; color: var(--text-secondary); text-align: center; margin-bottom: 0.3rem; font-weight: 600; }
.node { display: inline-flex; align-items: center; justify-content: center; padding: 0.55rem 1rem; border-radius: 6px; font-family: "Helvetica Neue", "Arial", sans-serif; font-size: 0.82rem; font-weight: 500; text-align: center; min-width: 120px; }
.node-primary { background: var(--diagram-node); color: var(--diagram-node-text); }
.node-secondary { background: var(--diagram-node-alt); color: var(--diagram-node-text); }
.node-highlight { background: var(--diagram-node-highlight); color: var(--diagram-node-text); }
.node-warn { background: var(--diagram-node-warn); color: var(--diagram-node-text); }
.node-outline { background: #fff; color: var(--diagram-node); border: 2px solid var(--diagram-node); }
.arrow-down { display: flex; justify-content: center; color: var(--diagram-edge); font-size: 1.2rem; }
.flow-row { display: flex; align-items: center; justify-content: center; gap: 0.3rem; flex-wrap: wrap; margin-bottom: 0.6rem; }
.flow-arrow { color: var(--diagram-edge); font-size: 1.1rem; font-weight: bold; }
.flow-node { display: inline-flex; align-items: center; justify-content: center; padding: 0.4rem 0.7rem; border-radius: 5px; font-family: "Helvetica Neue", "Arial", sans-serif; font-size: 0.78rem; font-weight: 500; white-space: nowrap; }

.section-number { color: var(--accent); font-weight: 700; margin-right: 0.3rem; }

/* Agent profile cards */
.agent-profile {
  background: var(--bg-alt);
  border: 1px solid var(--border-light);
  border-radius: 8px;
  padding: 1.3rem 1.5rem;
  margin: 1.2rem 0 1.8rem;
}

.agent-profile h4 { margin-top: 0; color: var(--accent); }
.agent-profile .agent-meta { font-size: 0.88rem; color: var(--text-secondary); margin-bottom: 0.6rem; font-family: "Helvetica Neue", "Arial", sans-serif; }
.agent-profile .agent-greeting { font-style: italic; color: var(--text-secondary); margin-top: 0.7rem; padding-left: 1rem; border-left: 3px solid var(--accent-light); }

@media print {
  body { max-width: 100%; padding: 0; font-size: 11pt; }
  .toc { break-after: page; }
  h2 { break-before: page; }
  table, .diagram, .agent-profile { break-inside: avoid; }
  a { color: var(--text); }
  .title-page { padding: 6rem 0 4rem; }
}
</style>
</head>
<body>

<div class="title-page">
  <h1>LAILA: Architecture of a Multi-Agent Tutoring System</h1>
  <p class="subtitle">Pedagogical Design, Agent Personas, Routing Strategies, and Learning Analytics</p>
  <p class="meta">Technical Report &mdash; February 2026</p>
</div>

<nav class="toc">
  <h2>Table of Contents</h2>
  <ol>
    <li><a href="#sec-intro">Introduction &amp; Pedagogical Rationale</a></li>
    <li><a href="#sec-arch">System Architecture</a></li>
    <li><a href="#sec-personas">The Built-In Agent Ensemble</a>
      <ol>
        <li><a href="#sec-personas-tutors">Professional Tutor Personas</a></li>
        <li><a href="#sec-personas-peers">Peer Student Personas</a></li>
        <li><a href="#sec-personas-design">Design Principles Across the Ensemble</a></li>
      </ol>
    </li>
    <li><a href="#sec-builder">The Student Agent Builder</a>
      <ol>
        <li><a href="#sec-builder-identity">Identity and Role Selection</a></li>
        <li><a href="#sec-builder-behavior">Behavioral Configuration</a></li>
        <li><a href="#sec-builder-blocks">Prompt Building Blocks</a></li>
        <li><a href="#sec-builder-test">Testing, Iteration, and Submission</a></li>
      </ol>
    </li>
    <li><a href="#sec-roles">Pedagogical Roles &amp; Personality System</a></li>
    <li><a href="#sec-deploy">Agent Deployment Across Learning Surfaces</a>
      <ol>
        <li><a href="#sec-deploy-tutor">Global Tutoring Conversations</a></li>
        <li><a href="#sec-deploy-course">Course-Level Tutors</a></li>
        <li><a href="#sec-deploy-forum">Forum AI Participation</a></li>
        <li><a href="#sec-deploy-section">Lecture-Embedded Chatbots</a></li>
        <li><a href="#sec-deploy-helper">Lecture AI Helper: Explain &amp; Discuss</a></li>
      </ol>
    </li>
    <li><a href="#sec-routing">Pedagogical Routing: Matching Students to Agents</a>
      <ol>
        <li><a href="#sec-routing-manual">Manual Selection</a></li>
        <li><a href="#sec-routing-router">Intelligent Routing</a></li>
        <li><a href="#sec-routing-collab">Collaborative Multi-Agent Responses</a></li>
        <li><a href="#sec-routing-random">Random Encounter</a></li>
      </ol>
    </li>
    <li><a href="#sec-prompt">Prompt Construction and Pedagogical Framing</a></li>
    <li><a href="#sec-logging">Learning Analytics and Behavioral Observation</a></li>
    <li><a href="#sec-metrics">Design Process Analytics</a></li>
    <li><a href="#sec-generation">AI-Powered Content Generation</a>
      <ol>
        <li><a href="#sec-generation-mcq">MCQ Generation for Quizzes</a></li>
        <li><a href="#sec-generation-practice">Practice Question Generation</a></li>
        <li><a href="#sec-generation-survey">Survey Generation</a></li>
      </ol>
    </li>
    <li><a href="#sec-assessment">The Assessment Engine</a>
      <ol>
        <li><a href="#sec-assessment-architecture">Quiz Architecture</a></li>
        <li><a href="#sec-assessment-types">Question Types and Grading</a></li>
        <li><a href="#sec-assessment-student">Student Experience</a></li>
      </ol>
    </li>
    <li><a href="#sec-labs">Custom Labs and Code Execution</a>
      <ol>
        <li><a href="#sec-labs-types">Lab Types and Templates</a></li>
        <li><a href="#sec-labs-execution">Code Execution Environment</a></li>
        <li><a href="#sec-labs-deployment">Lab Assignment and Deployment</a></li>
      </ol>
    </li>
    <li><a href="#sec-llm">Multi-Provider LLM Infrastructure</a>
      <ol>
        <li><a href="#sec-llm-abstraction">Provider Abstraction Layer</a></li>
        <li><a href="#sec-llm-providers">Supported Providers</a></li>
        <li><a href="#sec-llm-config">Configuration and Resilience</a></li>
      </ol>
    </li>
    <li><a href="#sec-safety">Content Safety and Integrity</a></li>
  </ol>
</nav>

<!-- ================================================================
     1. INTRODUCTION
     ================================================================ -->
<h2 id="sec-intro"><span class="section-number">1.</span> Introduction &amp; Pedagogical Rationale</h2>

<p>LAILA (Learning Analytics and Intelligent Learning Assistant) is a learning management system built around a central conviction: that effective AI tutoring requires not a single assistant but a <em>community</em> of pedagogically distinct agents, each embodying a different teaching philosophy, personality, and relationship to the learner. Where most educational platforms offer a single chatbot with a generic helpful disposition, LAILA provides a full ensemble of seven pre-built tutor personas&mdash;three professional guides and four simulated peer students&mdash;alongside a builder that allows students themselves to design new agents as an act of learning.</p>

<p>This dual-agent paradigm reflects two complementary educational goals. The first is <strong>adaptive support</strong>: different students, in different emotional states, working on different kinds of problems, benefit from different kinds of help. A student who is frustrated and considering giving up needs a different interlocutor than one who is intellectually restless and wants their assumptions challenged. By maintaining multiple agents with distinct personalities, LAILA can match the tutoring style to the student's needs&mdash;either through the student's own selection or through automatic routing based on the language and emotional tenor of their message.</p>

<p>The second goal is <strong>learning through design</strong>. When students build their own tutoring agents&mdash;choosing a pedagogical role, defining behavioral rules, crafting a system prompt, testing conversations, iterating on the design&mdash;they are engaged in a form of metacognitive exercise. Designing a tutor requires the student to articulate what good tutoring looks like: what questions a tutor should ask, what it should avoid saying, how it should handle frustration, when it should challenge and when it should console. This design process is itself a pedagogical intervention, graded as coursework and instrumented for research.</p>

<p>The agent architecture is not confined to a single chat window. The same agent persona appears across five distinct interaction surfaces within the platform: global tutoring conversations, course-specific tutor assignments, discussion forum threads, chatbot sections embedded directly in lecture content, and a two-mode lecture AI helper (explain and discuss). This omnipresence means that a student who has developed a relationship with Carmen&mdash;a casual peer who took the course last semester&mdash;can encounter Carmen in the tutoring chat, in a forum discussion, or assigned as a course-level tutor, each time with the same personality but grounded in the specific context of that interaction.</p>

<p>The technology stack uses a React frontend, an Express.js backend with PostgreSQL, and a multi-provider LLM integration layer supporting OpenAI, Anthropic, Google Gemini, and Ollama. The system supports four languages (English, Finnish, Arabic, Spanish) with full right-to-left layout support for Arabic. However, the emphasis of this report is on the pedagogical architecture&mdash;the decisions about who the agents are, how they behave, how they are assembled, and how their interactions with students are observed and analyzed&mdash;rather than on implementation details.</p>

<!-- ================================================================
     2. ARCHITECTURE
     ================================================================ -->
<h2 id="sec-arch"><span class="section-number">2.</span> System Architecture</h2>

<div class="diagram">
  <div class="diagram-title">Figure 1 &mdash; Three-Tier System Architecture</div>
  <div class="tier-diagram">
    <div class="tier-label">Student Interface</div>
    <div class="tier">
      <div class="node node-primary">Tutoring Chat</div>
      <div class="node node-primary">Course Tutors</div>
      <div class="node node-primary">Forum Discussions</div>
      <div class="node node-primary">Lecture Content</div>
    </div>
    <div class="arrow-down">&darr;</div>
    <div class="tier-label">Pedagogical Services Layer</div>
    <div class="tier">
      <div class="node node-secondary">Routing Engine</div>
      <div class="node node-secondary">Prompt Assembly</div>
      <div class="node node-secondary">Persona Management</div>
      <div class="node node-secondary">Conversation Memory</div>
    </div>
    <div class="arrow-down">&darr;</div>
    <div class="tier-label">AI &amp; Data Foundation</div>
    <div class="tier">
      <div class="node node-warn">OpenAI</div>
      <div class="node node-warn">Anthropic</div>
      <div class="node node-warn">Gemini</div>
      <div class="node node-warn">Ollama</div>
      <div class="node node-highlight">Learning Analytics Store</div>
    </div>
  </div>
</div>

<p>At the center of the system sits a single, reusable agent definition. Each agent&mdash;whether the Socratic Guide, Carmen, or a student-designed writing coach&mdash;is defined once and then deployed across every interaction surface in the platform. When an instructor assigns the Socratic Guide to their course with a customized name and adjusted system prompt, the system merges these course-specific overrides with the agent's global personality at runtime, preserving the core character while adapting to the course context. When that same agent appears in a forum thread, the thread's conversation history is injected as context so the agent responds in awareness of what other students have already said.</p>

<p>Every AI interaction, regardless of surface, passes through the same pipeline: the student's message is received, the appropriate agent is selected (manually, automatically, or collaboratively), a system prompt is assembled from the agent's persona definition plus context-specific augmentations, conversation history is retrieved, and the composite prompt is sent to the configured LLM provider. The response is then persisted, logged across multiple analytics layers, and returned to the student. The multi-provider abstraction means the platform can switch between OpenAI, Anthropic, Gemini, or a local Ollama instance without changing any pedagogical logic.</p>

<!-- ================================================================
     3. BUILT-IN AGENT ENSEMBLE
     ================================================================ -->
<h2 id="sec-personas"><span class="section-number">3.</span> The Built-In Agent Ensemble</h2>

<p>LAILA ships with seven pre-built agents organized into two pedagogically motivated categories. The first category&mdash;three <strong>professional tutors</strong>&mdash;represents distinct teaching philosophies: the Socratic method, direct instruction, and project-based coaching. These agents use formal or semi-formal registers and position themselves as knowledgeable guides. The second category&mdash;four <strong>peer student personas</strong>&mdash;simulates classmates at roughly the same level as the learner. These agents use casual language, admit uncertainty, give hints rather than answers, and emphasize the social and emotional dimensions of learning. The division reflects research suggesting that students engage differently with authority figures than with peers, and that peer-like interactions can reduce anxiety and increase willingness to ask questions.</p>

<p>The following table summarizes the configuration of all seven agents.</p>

<table>
  <thead>
    <tr>
      <th>Agent</th>
      <th>Category</th>
      <th>Personality</th>
      <th>Temp.</th>
      <th>Response Style</th>
      <th>Pedagogical Function</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>Socratic Guide</td><td>Tutor</td><td>Socratic</td><td>0.7</td><td>Balanced</td><td>Guided questioning, self-discovery</td></tr>
    <tr><td>Helpful Guide</td><td>Tutor</td><td>Friendly</td><td>0.6</td><td>Detailed</td><td>Clear explanation, step-by-step instruction</td></tr>
    <tr><td>Project Coach</td><td>Tutor</td><td>Professional</td><td>0.5</td><td>Detailed</td><td>Task decomposition, practical guidance</td></tr>
    <tr><td>Carmen</td><td>Peer</td><td>Casual</td><td>0.8</td><td>Concise</td><td>Hints and nudges from past experience</td></tr>
    <tr><td>Laila</td><td>Peer</td><td>Thoughtful</td><td>0.7</td><td>Concise</td><td>Cognitive conflict, respectful challenge</td></tr>
    <tr><td>Beatrice</td><td>Peer</td><td>Supportive</td><td>0.75</td><td>Balanced</td><td>Emotional support, validation, persistence</td></tr>
    <tr><td>Study Buddy</td><td>Peer</td><td>Casual</td><td>0.8</td><td>Concise</td><td>Co-learning, social studying</td></tr>
  </tbody>
</table>

<h3 id="sec-personas-tutors">3.1 Professional Tutor Personas</h3>

<div class="agent-profile">
  <h4>The Socratic Guide</h4>
  <div class="agent-meta">Personality: Socratic &middot; Temperature: 0.7 &middot; Response style: Balanced</div>
  <p>The Socratic Guide never gives direct answers. Its entire pedagogical approach is built around questioning: it asks probing follow-ups, uses "What do you think would happen if...?" formulations, and celebrates the moment when a student reaches understanding on their own. When a student asks "What is recursion?", the Socratic Guide does not define it. Instead, it might ask: "Have you ever placed two mirrors facing each other? What happens to the reflections?" It builds on whatever the student already knows, guiding them through a sequence of questions that leads to the concept rather than delivering it.</p>
  <p>The agent is explicitly instructed to never give answers directly unless the student is truly stuck after multiple attempts. It is told to build on the student's existing knowledge, help them see connections between concepts, and praise good reasoning when it appears. Its behavioral constraints prohibit lecturing without interaction, rushing to conclusions, or making the student feel wrong for an incorrect answer.</p>
  <p>The temperature of 0.7 gives it enough creative variance to formulate diverse questions across conversations, while remaining grounded enough that its questions stay pedagogically relevant rather than wandering into tangential territory.</p>
  <div class="agent-greeting">"Hello! I'm here to help you think through problems. What would you like to explore together?"</div>
</div>

<div class="agent-profile">
  <h4>The Helpful Guide</h4>
  <div class="agent-meta">Personality: Friendly &middot; Temperature: 0.6 &middot; Response style: Detailed</div>
  <p>Where the Socratic Guide withholds answers, the Helpful Guide provides them freely&mdash;but with structure, clarity, and care. This agent represents the direct instruction paradigm: it gives clear, step-by-step explanations, uses examples and analogies to make abstract concepts concrete, and checks for understanding after each explanation. When a student asks "What is recursion?", the Helpful Guide defines it, provides a simple example (factorial), then walks through the base case and recursive case, and finishes by asking whether the explanation made sense.</p>
  <p>The Helpful Guide's behavioral rules emphasize patience and thoroughness. It is instructed to explain clearly, use concrete examples, structure information logically, and never skip important steps. It is told not to use jargon without explaining it, not to rush through explanations, and not to be vague. Its response style is set to "detailed," meaning it provides comprehensive responses with examples, and its temperature of 0.6&mdash;slightly lower than the Socratic Guide's&mdash;reflects the need for precision in explanatory content. There is less room for creative improvisation when the goal is accurate explanation.</p>
  <div class="agent-greeting">"Hi there! I'm here to help explain things clearly. What can I help you understand?"</div>
</div>

<div class="agent-profile">
  <h4>The Project Coach</h4>
  <div class="agent-meta">Personality: Professional &middot; Temperature: 0.5 &middot; Response style: Detailed</div>
  <p>The Project Coach is the most practically oriented agent in the ensemble. It approaches learning as applied work: projects, assignments, code, deliverables. Its pedagogical philosophy is task decomposition&mdash;breaking large, intimidating tasks into manageable pieces, then helping the student work through each piece systematically. When a student says "I need to build a web application for my final project and I don't know where to start," the Project Coach begins by asking about requirements, then suggests a structured plan, helps prioritize, and focuses on the immediate next step.</p>
  <p>This agent debugs problems systematically rather than guessing. It suggests best practices and patterns. It provides practical, hands-on guidance. Its behavioral constraints include avoiding vague deliverables, not ignoring deadlines, never skipping the planning phase, and not overcomplicating solutions. The temperature of 0.5 is the lowest in the ensemble, reflecting the need for precise, actionable advice when dealing with project work&mdash;a domain where creative flourishes in language matter less than accuracy and specificity.</p>
  <div class="agent-greeting">"Ready to work on your project! What are we building today?"</div>
</div>

<h3 id="sec-personas-peers">3.2 Peer Student Personas</h3>

<p>The four peer personas represent the system's most distinctive pedagogical innovation. Rather than simulating an expert who has all the answers, these agents simulate classmates who have been through similar material and can offer hints, emotional support, and intellectual companionship&mdash;but not authoritative instruction. The pedagogical reasoning is drawn from Vygotsky's zone of proximal development: a peer who is slightly ahead can provide the kind of scaffolding that is most accessible to the learner, precisely because the peer recently occupied the same position of uncertainty. Each peer persona embodies a different dimension of peer support.</p>

<div class="agent-profile">
  <h4>Carmen</h4>
  <div class="agent-meta">Personality: Casual &middot; Temperature: 0.8 &middot; Response style: Concise</div>
  <p>Carmen is a fellow student who took this course last semester. She is not a tutor, not an expert, not a guide&mdash;she is a classmate who remembers struggling with the same material and is willing to chat about it in the library. Her language is casual and scattered with the kind of filler words real students use: "yeah," "honestly," "tbh," "like." She shares her own experience frequently: "When I took this, I also got confused by that part."</p>
  <p>The pedagogical design behind Carmen is deliberate restraint. She gives <em>hints</em> and <em>nudges</em>, not complete answers. She says things like "I remember the key thing was something about..." and "Have you tried looking at..." She points students in the right direction without doing the work for them. When she is not sure, she says so: "I think it's something like... but double-check that." This honesty about her own limitations models a growth mindset and normalizes the experience of not knowing.</p>
  <p>Carmen's behavioral constraints are precise: she must never give complete, polished answers; must never lecture or explain formally; must never pretend to be an expert; and must never solve homework for the student. Her high temperature of 0.8 generates the conversational variability that makes her feel like a real person&mdash;less predictable, more spontaneous, occasionally going on small tangents the way a real classmate would.</p>
  <div class="agent-greeting">"Hey! I took this class last semester so I might be able to help. What's giving you trouble?"</div>
</div>

<div class="agent-profile">
  <h4>Laila</h4>
  <div class="agent-meta">Personality: Thoughtful &middot; Temperature: 0.7 &middot; Response style: Concise</div>
  <p>Laila is the intellectual provocateur of the ensemble. She is a smart fellow student who loves a good discussion and is not afraid to push back on ideas&mdash;respectfully, warmly, but genuinely. When another student presents a position, Laila does not simply validate it. She challenges it: "Hmm, I'm not sure I agree with that... here's what I'm thinking." She plays devil's advocate, but in a way that is motivated by genuine curiosity rather than competitiveness. Her phrase vocabulary includes "But wait, what about...", "I see it differently...", "Have you considered...", and "Okay but here's where I push back a little..."</p>
  <p>The pedagogical theory behind Laila is cognitive conflict as a driver of learning. When a student's ideas are challenged&mdash;gently, by someone who clearly cares about them&mdash;they are forced to articulate and defend their reasoning, which deepens understanding. Laila's argumentative style is carefully constrained to be warm and exploratory rather than adversarial. She frames disagreements as joint exploration: "Let's think about this together..." She acknowledges strong points before introducing counterarguments: "That's true, AND..." And when the student makes a good defense, she celebrates it: "Okay, that's actually a good point!"</p>
  <p>Critically, Laila does not back down just to be nice. If she disagrees, she says so&mdash;kindly, but honestly. She also does not give complete answers, preferring to guide students to conclusions through dialectic. Her behavioral constraints prohibit being harsh or dismissive, arguing without purpose, giving complete solutions, or backing down merely to avoid conflict.</p>
  <div class="agent-greeting">"Hey! I love a good discussion. What are you working on? Fair warning&mdash;I might push back on some things, but it's all to help you think it through!"</div>
</div>

<div class="agent-profile">
  <h4>Beatrice</h4>
  <div class="agent-meta">Personality: Supportive &middot; Temperature: 0.75 &middot; Response style: Balanced</div>
  <p>Beatrice is the emotional anchor of the ensemble. She is an incredibly kind, patient, and endlessly encouraging fellow student who genuinely believes everyone can learn, no matter how behind they feel. Her primary function is affective support: when a student is frustrated, overwhelmed, or considering giving up, Beatrice is the agent most likely to help them stay.</p>
  <p>Beatrice's approach begins with validation. Before tackling any academic content, she acknowledges the student's emotional state: "I totally get why that's confusing." She normalizes struggle: "This topic is tough for everyone, you're not alone." She celebrates small wins: "That's great that you got that part!" She uses warm, reassuring language: "Don't worry," "You've got this," "That's a good start!" When a student is frustrated, she addresses the feelings first, then the problem&mdash;recognizing that emotional barriers to learning must be removed before cognitive engagement is possible.</p>
  <p>Like the other peer agents, Beatrice does not give complete answers. She wants the student to have the victory of figuring it out: "The first step is... can you figure out what comes next?" She breaks problems into tiny, manageable pieces. She reminds students of what they <em>do</em> understand before addressing what they don't. She never rushes, never shows impatience, and never makes anyone feel stupid or behind. Her kindness is described in her prompt as "genuine"&mdash;a deliberate distinction from superficial pleasantness.</p>
  <div class="agent-greeting">"Hi there! I'm here if you need any help. Don't worry, we'll figure it out together!"</div>
</div>

<div class="agent-profile">
  <h4>Study Buddy</h4>
  <div class="agent-meta">Personality: Casual &middot; Temperature: 0.8 &middot; Response style: Concise</div>
  <p>The Study Buddy is the most general of the peer personas: a friendly fellow student who approaches learning as a collaborative activity. Unlike Carmen (who positions herself as having completed the course), the Study Buddy learns alongside the student, using phrases like "I think..." and "Let's figure this out together." It shares its own understanding honestly, admits when it is unsure, and frames studying as a conversation with a friend rather than a transaction with an expert.</p>
  <p>The Study Buddy's pedagogical contribution is social learning&mdash;the idea that thinking out loud with another person, even one who does not know more than you, can clarify understanding. It uses casual language and everyday expressions, relates academic topics to common experiences, and treats the learning process as something shared and enjoyable rather than individual and stressful. Its behavioral constraints prevent it from sounding like a teacher, being condescending, using overly formal language, or pretending to know everything.</p>
  <div class="agent-greeting">"Hey! What are you working on? Let's figure it out together!"</div>
</div>

<h3 id="sec-personas-design">3.3 Design Principles Across the Ensemble</h3>

<p>Several design principles cut across all seven agents. First, every agent carries explicit <strong>do's and don'ts</strong>&mdash;lists of required and prohibited behaviors that constrain the output regardless of what the student asks. These rules encode each agent's pedagogical commitments at a granularity the system prompt alone cannot achieve. Without them, all agents would gradually converge on the same generic helpful behavior.</p>

<table>
  <thead>
    <tr>
      <th>Agent</th>
      <th>Do's</th>
      <th>Don'ts</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Socratic Guide</strong></td>
      <td>Ask clarifying questions; Build on student responses; Encourage self-discovery; Praise good reasoning; Use leading questions</td>
      <td>Give direct answers immediately; Lecture without interaction; Make the student feel wrong; Rush to conclusions</td>
    </tr>
    <tr>
      <td><strong>Helpful Guide</strong></td>
      <td>Explain clearly and thoroughly; Use concrete examples; Structure information logically; Be patient; Check for understanding</td>
      <td>Be vague or unclear; Skip important steps; Use jargon without explaining; Rush explanations</td>
    </tr>
    <tr>
      <td><strong>Project Coach</strong></td>
      <td>Create actionable task lists; Set realistic milestones; Debug systematically; Suggest best practices; Focus on practical solutions</td>
      <td>Be vague about deliverables; Ignore deadlines; Skip planning steps; Overcomplicate solutions</td>
    </tr>
    <tr>
      <td><strong>Carmen</strong></td>
      <td>Give hints not answers; Share own struggles; Be casual and friendly; Point in the right direction; Admit when unsure</td>
      <td>Give complete answers; Sound like a teacher; Do their work for them; Pretend to be an expert</td>
    </tr>
    <tr>
      <td><strong>Laila</strong></td>
      <td>Challenge ideas constructively; Offer alternative viewpoints; Be supportive while disagreeing; Push deeper thinking; Give hints not answers</td>
      <td>Be harsh or dismissive; Give complete solutions; Argue without purpose; Back down just to be nice</td>
    </tr>
    <tr>
      <td><strong>Beatrice</strong></td>
      <td>Be warm and encouraging; Validate feelings; Celebrate small wins; Give gentle hints; Break things into small steps</td>
      <td>Make them feel bad; Give complete answers; Rush or show impatience; Be condescending</td>
    </tr>
    <tr>
      <td><strong>Study Buddy</strong></td>
      <td>Be casual and friendly; Share relatable experiences; Encourage and support; Learn together; Use everyday language</td>
      <td>Sound like a teacher; Be condescending; Use overly formal language; Pretend to know everything</td>
    </tr>
  </tbody>
</table>

<p>Second, each agent's <strong>temperature</strong> is calibrated to its pedagogical personality. Peer agents operate at higher temperatures (0.75&ndash;0.8) because casual, spontaneous language is essential to the illusion of a real classmate. The Project Coach operates at the lowest (0.5) because precision matters most in actionable project advice. The Socratic Guide sits in the middle (0.7)&mdash;creative enough to formulate diverse questions, grounded enough to stay pedagogically relevant.</p>

<p>Third, all agents produce <strong>short responses</strong>&mdash;300 to 500 characters, roughly two to three sentences. Real tutors do not deliver paragraphs. They say something brief, pause, and wait for the student to react. The short response length enforces a conversational rhythm and prevents verbose, essay-like output that would break the illusion of dialogue.</p>

<p>Finally, all agents carry a critical instruction to <strong>never prefix their response with their own name</strong>. The UI already displays the agent's name and avatar, so "Beatrice: Well, I think..." reads as redundant. A post-processing step strips name prefixes as a safety net.</p>

<!-- ================================================================
     4. STUDENT AGENT BUILDER
     ================================================================ -->
<h2 id="sec-builder"><span class="section-number">4.</span> The Student Agent Builder</h2>

<p>The agent builder turns the act of designing a tutoring agent into a pedagogical exercise. Students do not merely use AI tutors; they create them. This student-as-designer paradigm requires learners to articulate their understanding of what makes tutoring effective&mdash;what a good tutor should say, how it should say it, when it should challenge and when it should support, what knowledge it needs and what it should admit it lacks. The builder is structured as a graded coursework assignment: instructors create agent-design assignments, students build and test agents over a period of time, then submit their designs for review.</p>

<h3 id="sec-builder-identity">4.1 Identity and Role Selection</h3>

<p>The builder begins with identity. Students choose from ten pedagogical role templates&mdash;Peer Tutor, Study Buddy, Socratic Guide, Writing Coach, Research Assistant, Debate Partner, Concept Explainer, Practice Interviewer, Language Partner, and Problem-Solving Coach. Each template pre-populates a complete starting configuration: a system prompt describing the role's pedagogical approach, recommended do's and don'ts, a suggested personality, and an example welcome message. The student then personalizes this foundation by naming their agent, giving it a title (e.g., "Alex the Writing Coach"), writing a persona description, and selecting an avatar.</p>

<p>Role selection is scaffolding, not a constraint. Students can modify every pre-populated field, overriding the template defaults entirely. The template gives students who have never thought about prompt engineering a pedagogically grounded starting point; advanced students can build from scratch.</p>

<table>
  <thead>
    <tr>
      <th>Role Template</th>
      <th>Default Personality</th>
      <th>Learning Theory</th>
      <th>Core Approach</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>Peer Tutor</td><td>Friendly</td><td>Collaborative learning</td><td>Explain at the student's level, celebrate shared progress</td></tr>
    <tr><td>Study Buddy</td><td>Encouraging</td><td>Social learning</td><td>Learn alongside the student, mutual exploration</td></tr>
    <tr><td>Socratic Guide</td><td>Socratic</td><td>Maieutic method</td><td>Guide through questioning, never give direct answers</td></tr>
    <tr><td>Writing Coach</td><td>Professional</td><td>Formative feedback</td><td>Specific, actionable suggestions; preserve student voice</td></tr>
    <tr><td>Research Assistant</td><td>Academic</td><td>Information literacy</td><td>Source evaluation, search strategy, proper attribution</td></tr>
    <tr><td>Debate Partner</td><td>Professional</td><td>Critical thinking</td><td>Respectful argumentation, identify logical fallacies</td></tr>
    <tr><td>Concept Explainer</td><td>Friendly</td><td>Analogical reasoning</td><td>Metaphors, real-world connections, concrete examples</td></tr>
    <tr><td>Practice Interviewer</td><td>Professional</td><td>Simulation-based learning</td><td>Mock interviews with structured feedback</td></tr>
    <tr><td>Language Partner</td><td>Friendly</td><td>Communicative language teaching</td><td>Natural conversation with gentle correction</td></tr>
    <tr><td>Problem-Solving Coach</td><td>Socratic</td><td>Metacognition</td><td>Problem decomposition, strategy selection, reflection</td></tr>
  </tbody>
</table>

<h3 id="sec-builder-behavior">4.2 Behavioral Configuration</h3>

<p>The behavior tab defines <em>how</em> the agent interacts. Students select a personality preset from seven options, each carrying a pre-written prompt that shapes tone and communication style.</p>

<table>
  <thead>
    <tr>
      <th>Personality Preset</th>
      <th>Tone</th>
      <th>Typical Use</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>Friendly</td><td>Warm, approachable, celebratory</td><td>Peer tutors, concept explainers</td></tr>
    <tr><td>Professional</td><td>Polished, formal, business-appropriate</td><td>Writing coaches, interviewers, debate partners</td></tr>
    <tr><td>Socratic</td><td>Question-first, discovery-oriented</td><td>Socratic guides, problem-solving coaches</td></tr>
    <tr><td>Encouraging</td><td>Highly supportive, motivating</td><td>Study buddies, emotional support roles</td></tr>
    <tr><td>Academic</td><td>Scholarly, precise, educational</td><td>Research assistants, subject experts</td></tr>
    <tr><td>Casual</td><td>Relaxed, conversational, easy-going</td><td>Peer helpers, casual study partners</td></tr>
    <tr><td>Custom</td><td>Student-defined free text</td><td>Advanced design exercise</td></tr>
  </tbody>
</table>

<p>Students also set a response style (concise, balanced, or detailed), construct lists of do's and don'ts, and write suggested conversation starters. Each decision requires the student to think through the pedagogical experience they want to create.</p>

<h3 id="sec-builder-blocks">4.3 Prompt Building Blocks</h3>

<p>For students who find writing raw system prompts intimidating, the builder provides a library of 38 pre-written prompt building blocks organized into six categories: Persona (who the agent is), Tone and Style (how it communicates), Behaviors (what it does), Constraints (what it avoids), Response Format (how it structures answers), and Knowledge Bounds (the scope of its expertise). Each block is a single, self-contained prompt instruction&mdash;for example, "Ask follow-up questions to check understanding and clarify needs" or "Do not simply give direct answers; guide the student to discover answers themselves."</p>

<p>Students select blocks by clicking tiles, and the system composes them into a structured prompt automatically. The composition follows a defined assembly order, making the invisible craft of prompt engineering visible and modular.</p>

<table>
  <thead>
    <tr>
      <th>Category</th>
      <th>Blocks</th>
      <th>Assembly Position</th>
      <th>Example Blocks</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>Persona</td><td>6</td><td>1st &mdash; "You are..."</td><td>Patient Tutor, Study Partner, Coach, Mentor, Subject Expert, Peer Helper</td></tr>
    <tr><td>Tone &amp; Style</td><td>6</td><td>2nd &mdash; voice and register</td><td>Encouraging, Casual &amp; Friendly, Professional, Patient &amp; Calm, Enthusiastic, Direct &amp; Clear</td></tr>
    <tr><td>Behaviors</td><td>10</td><td>3rd &mdash; "When helping students:"</td><td>Ask Follow-up Questions, Give Examples, Break Down Complex Topics, Use Analogies, Check Understanding</td></tr>
    <tr><td>Constraints</td><td>8</td><td>4th &mdash; "Important guidelines:"</td><td>Don't Give Direct Answers, Avoid Jargon, Don't Be Condescending, Don't Assume Knowledge, Admit Limitations</td></tr>
    <tr><td>Response Format</td><td>6</td><td>5th &mdash; "Response formatting:"</td><td>Use Bullet Points, Numbered Steps, Use Headers, Keep Concise, Provide Detail, End with Summary</td></tr>
    <tr><td>Knowledge Bounds</td><td>6</td><td>6th &mdash; expertise scope</td><td>Beginner Level, Intermediate Level, Focus on Concepts, Practical Applications, Redirect Off-Topic</td></tr>
  </tbody>
</table>

<h3 id="sec-builder-test">4.4 Testing, Iteration, and Submission</h3>

<p>The fourth tab provides a live testing environment where students converse with their agent using the current configuration. Each test session records a full snapshot of the configuration, creating a versioned history that lets both the student and the instructor trace how the design evolved.</p>

<div class="diagram">
  <div class="diagram-title">Figure 2 &mdash; Agent Builder Architecture</div>
  <div class="tier-diagram">
    <div class="tier-label">Builder Tabs</div>
    <div class="tier">
      <div class="node node-primary">Identity &amp; Role</div>
      <div class="node node-primary">Behavior &amp; Personality</div>
      <div class="node node-primary">Prompt Blocks</div>
      <div class="node node-primary">Test &amp; Iterate</div>
    </div>
    <div class="arrow-down">&darr;</div>
    <div class="tier-label">Configuration</div>
    <div class="tier">
      <div class="node node-secondary">10 Role Templates</div>
      <div class="node node-secondary">7 Personality Presets</div>
      <div class="node node-secondary">38 Prompt Blocks</div>
      <div class="node node-secondary">Do's &amp; Don'ts</div>
    </div>
    <div class="arrow-down">&darr;</div>
    <div class="tier-label">Output</div>
    <div class="tier">
      <div class="node node-highlight">Assembled System Prompt</div>
      <div class="node node-warn">Config Snapshot + Version</div>
    </div>
  </div>
</div>

<div class="diagram">
  <div class="diagram-title">Figure 2b &mdash; Design &amp; Submission Lifecycle</div>
  <div class="flow-row">
    <div class="flow-node node-primary">Select Role</div>
    <span class="flow-arrow">&rarr;</span>
    <div class="flow-node node-secondary">Configure</div>
    <span class="flow-arrow">&rarr;</span>
    <div class="flow-node node-secondary">Test</div>
    <span class="flow-arrow">&rarr;</span>
    <div class="flow-node node-secondary">Iterate</div>
    <span class="flow-arrow">&rarr;</span>
    <div class="flow-node node-highlight">Submit</div>
    <span class="flow-arrow">&rarr;</span>
    <div class="flow-node node-warn">Instructor Review</div>
  </div>
</div>

<p>The design loop is intentionally iterative. Students configure their agent, test it, observe how it behaves, then return to the configuration to adjust. Each return counts as an iteration, and the system tracks the total number of iterations, the total design time in seconds, and the number of test conversations conducted. These metrics serve both as grading signals (an agent that was tested extensively and iterated many times likely reflects deeper engagement) and as research data about the design process itself.</p>

<p>Upon submission, the agent moves from draft to submitted status. Instructors review the submission through a dedicated interface that shows the final configuration, the full history of test conversations (each tagged with the configuration version that was active at the time), the chronological trail of configuration changes with before-and-after snapshots, and the student's reflection responses. This creates a rich artifact for assessment: not just the final product, but the <em>process</em> of designing it.</p>

<!-- ================================================================
     5. PEDAGOGICAL ROLES & PERSONALITY SYSTEM
     ================================================================ -->
<h2 id="sec-roles"><span class="section-number">5.</span> Pedagogical Roles &amp; Personality System</h2>

<p>The ten pedagogical roles available in the builder represent a curated taxonomy of tutoring strategies, each grounded in a distinct learning theory tradition. Each role ships with a complete default configuration: a multi-paragraph system prompt, four to five recommended do's, four don'ts, a suggested personality preset, and an example welcome message.</p>

<p>The <strong>personality presets</strong> operate independently from roles. A student designing a Debate Partner might select the "professional" personality for a formal exchange, or "casual" for something resembling a late-night dorm room argument. This independence is deliberate: the role defines <em>what</em> the agent does, while the personality defines <em>how</em> it communicates.</p>

<p>The interplay between these dimensions creates a multidimensional configuration space. The following table illustrates how the five configuration axes combine.</p>

<table>
  <thead>
    <tr>
      <th>Configuration Axis</th>
      <th>Options</th>
      <th>Controls</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>Pedagogical Role</td><td>10 templates</td><td>Teaching philosophy, default behaviors</td></tr>
    <tr><td>Personality Preset</td><td>7 options (6 preset + custom)</td><td>Tone, register, communication style</td></tr>
    <tr><td>Response Style</td><td>Concise / Balanced / Detailed</td><td>Verbosity, depth of explanation</td></tr>
    <tr><td>Prompt Building Blocks</td><td>42 blocks across 6 categories</td><td>Specific behavioral instructions</td></tr>
    <tr><td>Do's and Don'ts</td><td>Free-form lists</td><td>Hard behavioral constraints</td></tr>
    <tr><td>Temperature</td><td>0.0 &ndash; 1.0 (slider)</td><td>Creativity vs. precision trade-off</td></tr>
  </tbody>
</table>

<p>Two agents with the same "Peer Tutor" role can behave very differently: one with a casual personality, high temperature, and constraints against giving direct answers; another with a professional personality, low temperature, and emphasis on structured explanations. This combinatorial richness forces students to think about tutoring as a multifaceted practice, not a single skill.</p>

<!-- ================================================================
     6. DEPLOYMENT ACROSS LEARNING SURFACES
     ================================================================ -->
<h2 id="sec-deploy"><span class="section-number">6.</span> Agent Deployment Across Learning Surfaces</h2>

<p>A defining feature of LAILA's multi-agent architecture is that agents are not confined to a single chat interface. The same persona appears in five contexts within the platform, each adapted to the specific pedagogical purpose of that surface. Students experience agents as persistent characters, not interchangeable chatbots.</p>

<table>
  <thead>
    <tr>
      <th>Surface</th>
      <th>Scope</th>
      <th>Context Injected</th>
      <th>History Window</th>
      <th>Visibility</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>Global Tutoring</td><td>Open-ended, any topic</td><td>Agent persona + routing metadata</td><td>Last 10 messages</td><td>Private to student</td></tr>
    <tr><td>Course Tutors</td><td>Course-specific</td><td>Instructor overrides + course title</td><td>Last 20 messages</td><td>Private to student</td></tr>
    <tr><td>Forum AI</td><td>Thread-specific</td><td>Full thread history + author names</td><td>Entire thread</td><td>Public to course</td></tr>
    <tr><td>Lecture Chatbots</td><td>Section-specific</td><td>Course &rarr; Module &rarr; Lecture hierarchy</td><td>Last 20 messages</td><td>Private to student</td></tr>
    <tr><td>Lecture AI Helper</td><td>Lecture content</td><td>Extracted text + PDFs + mode (explain/discuss)</td><td>Thread history</td><td>Private to student</td></tr>
  </tbody>
</table>

<h3 id="sec-deploy-tutor">6.1 Global Tutoring Conversations</h3>

<p>The primary tutoring interface provides open-ended, multi-turn conversations with any active agent. Each student maintains a persistent session with separate conversation threads per agent, so switching from the Socratic Guide to Beatrice does not erase the conversation history with either. The system preserves the last ten messages as conversational context, giving agents memory of what has been discussed recently. This is where the four routing modes (manual, router, collaborative, random) are active, allowing students to choose how they want to be matched with agents. The global tutoring chat is the most general-purpose surface: students can bring any question, about any topic, in any emotional state, and the system's routing logic will attempt to connect them with the most appropriate agent.</p>

<h3 id="sec-deploy-course">6.2 Course-Level Tutors</h3>

<p>Instructors can assign any global agent to their specific course, creating a course-scoped tutoring relationship. The critical feature here is <strong>customization overrides</strong>: when assigning an agent to a course, the instructor can change its display name, rewrite its system prompt, adjust its personality, and modify its temperature&mdash;all without affecting the global agent definition. The system merges these overrides at runtime, preferring the instructor's customization where it exists and falling back to the global default where it does not.</p>

<p>This mechanism enables a powerful pattern: an instructor teaching a machine learning course can take the Helpful Guide&mdash;a generalist explainer&mdash;and turn it into an ML-specific tutor by adding a custom system prompt that references course terminology, common student misconceptions, and specific learning objectives. The agent retains the Helpful Guide's warm, patient personality and step-by-step explanation style, but its knowledge is now grounded in the course's domain. Students interact through multi-turn conversations scoped to the course, with the last twenty messages providing conversational context and the course title injected into the prompt for grounding.</p>

<h3 id="sec-deploy-forum">6.3 Forum AI Participation</h3>

<p>In discussion forums, agents participate as visible contributors to threaded conversations. When a student invokes an AI agent in a forum thread, the system retrieves the entire thread history&mdash;including author names, timestamps, and whether previous posts were human or AI-generated&mdash;and injects this context into the agent's prompt so that it responds in awareness of the full discussion. The resulting post is marked as AI-generated, tagged with the agent's name, and attributed to the student who requested it.</p>

<p>This is pedagogically distinct from the chat interfaces. In a forum, the agent's response is <em>public</em>&mdash;visible to every student in the course. The agent is therefore instructed to be educational and supportive in a way appropriate for a community setting: to ask follow-up questions that deepen the discussion, connect concepts to course material, keep responses focused, and acknowledge uncertainty when it arises. A student might invoke Carmen in a forum to get a peer-like perspective on a classmate's question, or the Socratic Guide to raise a challenging counterpoint that pushes the entire class's thinking forward.</p>

<h3 id="sec-deploy-section">6.4 Lecture-Embedded Chatbots</h3>

<p>Instructors can embed chatbot sections directly into lecture content, creating interactive agents that appear within the flow of a lesson. Each chatbot section carries its own system prompt, welcome message, title, and introductory text, allowing the instructor to create highly focused agents that address specific learning objectives within a particular lecture. For example, a lecture on sorting algorithms might include a chatbot section titled "Practice Explaining Merge Sort" with a system prompt that specifically challenges the student to explain the algorithm step by step.</p>

<p>Each student has a private, persistent conversation with the chatbot section. The system automatically injects the course title, module title, and lecture title into the prompt, grounding the agent's responses in the specific content the student is studying. Conversation history is limited to the last twenty messages to keep the context window focused.</p>

<h3 id="sec-deploy-helper">6.5 Lecture AI Helper: Explain &amp; Discuss</h3>

<p>The lecture AI helper provides two complementary pedagogical modes, both deeply integrated with the lecture's actual content.</p>

<p><strong>Explain mode</strong> provides direct, clear explanations of lecture material. The system constructs the AI's knowledge base by extracting text from the lecture's written sections, AI-generated content sections, and PDF attachments&mdash;including the ability for students to select specific page ranges from PDFs to focus the AI's context. The resulting prompt instructs the AI to explain concepts clearly, break down complex topics, highlight key takeaways, and reference specific parts of the lecture. This mode is for students who need help understanding what they have read.</p>

<p><strong>Discuss mode</strong> adopts a Socratic approach. Given the same lecture content, the AI asks probing questions rather than providing explanations: "Why do you think the author emphasizes this point?", "How does this connect to what we covered last week?", "What would happen if this assumption were false?" This mode is for students who have understood the surface content and need to be pushed toward deeper engagement.</p>

<p>Both modes operate through a thread-based question-and-answer model. Each question starts a new thread with an initial AI response, and students can post follow-up questions within the thread, with support for nested replies. The AI maintains awareness of the full thread history when responding to follow-ups, creating a coherent multi-turn exploration of the lecture content.</p>

<!-- ================================================================
     7. ROUTING
     ================================================================ -->
<h2 id="sec-routing"><span class="section-number">7.</span> Pedagogical Routing: Matching Students to Agents</h2>

<p>The global tutoring chat supports four routing modes, each embodying a different philosophy about how students should be matched with agents. Routing is configured at the session level; a student can switch modes at any time.</p>

<div class="diagram">
  <div class="diagram-title">Figure 2c &mdash; Routing Mode Overview</div>
  <div class="tier-diagram">
    <div class="tier-label">Student Message</div>
    <div class="tier">
      <div class="node node-primary" style="min-width: 200px;">Incoming Message</div>
    </div>
    <div class="arrow-down">&darr;</div>
    <div class="tier-label">Routing Mode Selection</div>
    <div class="tier">
      <div class="node node-secondary">Manual</div>
      <div class="node node-secondary">Router</div>
      <div class="node node-secondary">Collaborative</div>
      <div class="node node-secondary">Random</div>
    </div>
    <div class="arrow-down">&darr;</div>
    <div class="tier-label">Agent Selection</div>
    <div class="tier">
      <div class="node node-outline">Student picks</div>
      <div class="node node-outline">Keyword scoring</div>
      <div class="node node-outline">@mention / auto-select</div>
      <div class="node node-outline">Uniform random</div>
    </div>
    <div class="arrow-down">&darr;</div>
    <div class="tier-label">Response</div>
    <div class="tier">
      <div class="node node-highlight">1 agent responds</div>
      <div class="node node-highlight">1 agent responds</div>
      <div class="node node-highlight">2&ndash;3 agents respond</div>
      <div class="node node-highlight">1 agent responds</div>
    </div>
  </div>
</div>

<table>
  <thead>
    <tr>
      <th>Mode</th>
      <th>Selection Method</th>
      <th>Agents per Message</th>
      <th>Pedagogical Rationale</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>Manual</td><td>Student picks agent</td><td>1</td><td>Respects student agency and developed preferences</td></tr>
    <tr><td>Router</td><td>Keyword scoring or AI analysis</td><td>1</td><td>Matches tutoring style to student need</td></tr>
    <tr><td>Collaborative</td><td>@mention, picker, or auto</td><td>2&ndash;3</td><td>Multi-perspective comparison and synthesis</td></tr>
    <tr><td>Random</td><td>Uniform random</td><td>1</td><td>Exposure to unfamiliar agents and teaching styles</td></tr>
  </tbody>
</table>

<h3 id="sec-routing-manual">7.1 Manual Selection</h3>

<p>In manual mode, the student chooses which agent to talk to. This is the default for new sessions. It respects student agency: a student who has learned that Laila's challenges sharpen their thinking, or that Beatrice's encouragement helps them persist, can go directly to that agent. Students develop preferences and relationships with specific agents over time, and manual mode honors those relationships.</p>

<h3 id="sec-routing-router">7.2 Intelligent Routing</h3>

<p>Router mode automatically selects the best agent based on the content and emotional tone of the student's message. The default strategy is keyword-based and instantaneous: the system scores every available agent against the incoming message using seven categories of keyword matching. Every agent starts with a base score of 0.3, receives boosts when keywords in its categories are detected, and scores are capped at 0.95. The highest-scoring agent is selected.</p>

<table>
  <thead>
    <tr>
      <th>Category</th>
      <th>Example Keywords</th>
      <th>Primary Agents</th>
      <th>Boost</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>Emotional Support</td><td>frustrated, overwhelmed, give up, anxious, scared, stressed</td><td>Beatrice, Helpful Guide</td><td>+0.6</td></tr>
    <tr><td>Discussion / Debate</td><td>disagree, opinion, what do you think, debate, perspective</td><td>Laila, Socratic Guide</td><td>+0.5</td></tr>
    <tr><td>Conceptual Understanding</td><td>why, what if, explain, understand, concept, theory, meaning</td><td>Socratic Guide, Laila</td><td>+0.5</td></tr>
    <tr><td>Step-by-Step Guidance</td><td>how do, how to, show me, steps, guide, walk me through</td><td>Helpful Guide, Project Coach</td><td>+0.5</td></tr>
    <tr><td>Project / Coding</td><td>project, build, code, implement, debug, error, fix, bug</td><td>Project Coach, Helpful Guide</td><td>+0.5</td></tr>
    <tr><td>Casual Support</td><td>hey, hi, stuck, confused, lost, help me, quick question</td><td>Carmen, Beatrice</td><td>+0.4</td></tr>
    <tr><td>Encouragement / Beginners</td><td>trying, learning, new to, beginner, first time, not sure</td><td>Beatrice, Helpful Guide, Carmen</td><td>+0.4</td></tr>
  </tbody>
</table>

<p>An optional AI-based routing strategy uses the language model itself to analyze the student's message, considering emotional tone, question complexity, and the type of help needed. This produces more nuanced routing at the cost of an additional API call; the system falls back to keyword routing if AI analysis fails. The routing decision is logged with the selected agent, the reason for selection, a confidence score, and the scores of all alternative agents. This metadata is returned to the student, making the routing decision transparent.</p>

<h3 id="sec-routing-collab">7.3 Collaborative Multi-Agent Responses</h3>

<p>Collaborative mode invites multiple agents to respond to the same student message, creating a multi-perspective learning experience. Students see each agent's response separately, attributed and styled distinctly.</p>

<p>Agent selection follows a priority chain: (1) students can <strong>@mention</strong> agents by name in their message (e.g., "@Carmen @Laila, what do you think about this?"); (2) failing that, they pre-select agents through a picker; (3) if neither is used, the system auto-selects the two most relevant agents using keyword scoring.</p>

<p>Four interaction styles control how the agents relate to each other's responses.</p>

<table>
  <thead>
    <tr>
      <th>Style</th>
      <th>Mechanism</th>
      <th>Agents See Prior Responses?</th>
      <th>Rounds</th>
      <th>Pedagogical Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>Parallel</td><td>All agents respond simultaneously</td><td>No</td><td>1</td><td>Compare independent approaches to the same question</td></tr>
    <tr><td>Sequential</td><td>Agents respond in order</td><td>Yes (cumulative)</td><td>1</td><td>Build on prior perspectives; layered, cumulative response</td></tr>
    <tr><td>Debate</td><td>Two rounds of back-and-forth</td><td>Yes (per round)</td><td>2</td><td>Intellectual exchange; agents agree, disagree, add nuance</td></tr>
    <tr><td>Random</td><td>1&ndash;3 random agents, sequential</td><td>Yes (cumulative)</td><td>1</td><td>Surprise and variety; unpredictable multi-voice responses</td></tr>
  </tbody>
</table>

<p>In <strong>parallel style</strong>, each agent responds independently&mdash;useful for contrasting the Socratic Guide's questioning with the Helpful Guide's direct explanation. In <strong>sequential style</strong>, each successive agent sees what came before and builds on it, creating a cumulative response. In <strong>debate style</strong>, agents exchange two rounds: first sharing their perspective, then engaging with what the others said&mdash;agreeing, disagreeing, adding nuance. This is particularly powerful when agents with contrasting approaches participate: the Socratic Guide asks a probing question, Laila pushes back on the premise, and the student observes a genuine intellectual exchange. <strong>Random style</strong> selects one to three agents at random and chains their responses sequentially.</p>

<h3 id="sec-routing-random">7.4 Random Encounter</h3>

<p>Random mode selects a single agent uniformly at random for each message. The pedagogical value is exposure: a student who might always choose the Helpful Guide in manual mode is introduced to agents they would not have sought out&mdash;potentially discovering that the Socratic Guide's questioning or Carmen's hint-based style works better for certain problems.</p>

<!-- ================================================================
     8. PROMPT CONSTRUCTION
     ================================================================ -->
<h2 id="sec-prompt"><span class="section-number">8.</span> Prompt Construction and Pedagogical Framing</h2>

<p>The system prompt is the mechanism by which an agent's pedagogical identity is communicated to the language model. LAILA constructs these prompts through a layered assembly process that begins with the agent's core persona and progressively adds behavioral constraints, interaction guidelines, and context-specific augmentations.</p>

<p>The first layer is the <strong>base persona</strong>: the agent's own system prompt, which describes its character, teaching philosophy, and approach. For the Socratic Guide, this is a paragraph about asking probing questions, building on student knowledge, and never giving direct answers. For Carmen, it is a detailed character description of a casual classmate who gives hints in slang.</p>

<p>The second layer is <strong>identity enforcement</strong>. The system appends an explicit instruction to stay in character throughout the conversation, use "you" to address the student, and never begin a response with a name followed by a colon. This layer exists because language models tend to break character over long conversations, gradually reverting to a generic helpful assistant persona. The enforcement instructions counteract this drift.</p>

<p>The third layer is <strong>response formatting</strong>. All agents in the global tutoring chat are constrained to short responses (300-500 characters), plain text without markdown headers or tables, and direct, point-first communication. These constraints shape the interaction into a dialogue rather than a lecture&mdash;the agent says something brief, the student responds, and the conversation develops turn by turn.</p>

<p>The fourth layer is the agent's <strong>do's and don'ts</strong>, appended as bulleted lists. These are the most specific behavioral instructions: "Ask clarifying questions," "Celebrate when students reach understanding," "Never give the answer directly unless the student is truly stuck."</p>

<p>This four-layer construction is the baseline used in global tutoring. Each deployment surface then adds its own <strong>context-specific augmentation</strong>. Course-level tutors merge instructor overrides with the global persona and inject the course title. Lecture section chatbots inject the course, module, and lecture hierarchy. The lecture AI helper constructs a knowledge base from the lecture's actual content&mdash;extracted text, AI-generated sections, and PDF pages&mdash;and wraps it in either explain-mode instructions (clear explanation, concrete examples, key takeaways) or discuss-mode instructions (probing questions, critical thinking, guided discovery). Forum AI participation injects the full thread history with author attribution, so the agent is aware of what every participant&mdash;human and AI&mdash;has already said.</p>

<p>In collaborative mode, the prompt is further augmented per style. Sequential agents receive the prior agents' responses as context with instructions to build on them. Debate agents receive round-labeled context with instructions to engage with prior viewpoints. This context injection is what transforms multiple independent LLM calls into a coherent multi-agent conversation.</p>

<!-- ================================================================
     9. LEARNING ANALYTICS
     ================================================================ -->
<h2 id="sec-logging"><span class="section-number">9.</span> Learning Analytics and Behavioral Observation</h2>

<p>LAILA's logging architecture treats every student-agent interaction as a research observation. The system captures data at multiple layers of granularity, from individual message exchanges to session-level summaries to emotional self-reports, creating a comprehensive record of how students interact with AI tutors across every surface of the platform.</p>

<h3>9.1 Interaction-Level Logging</h3>

<p>Every message exchange in the global tutoring chat is logged with full context: which student sent the message, which agent responded, what routing mode was active, how the agent was selected (and with what confidence), what the student wrote, what the agent replied, how long the response took, which LLM model generated it, and what device the student was using. For collaborative responses, the individual contribution of each participating agent is recorded separately, including per-agent response times and the collaboration style used. For routed responses, the scoring of all alternative agents is logged alongside the selected one, enabling retrospective analysis of routing quality.</p>

<p>Lecture section chatbots generate an even richer log entry with over thirty fields capturing the full course hierarchy context (course, module, lecture, and section identifiers and titles at the time of interaction), conversation sequence tracking (which message number this was in the conversation), the complete chatbot configuration as a snapshot, word and character counts for both the student message and the AI response, browser and device characteristics, session timing, and an optional test-mode flag for when administrators are testing the interface in different roles.</p>

<h3>9.2 Unified Activity Logging</h3>

<p>Above the interaction-level logs sits a unified activity logging layer inspired by the xAPI (Experience API) standard. Every meaningful student action&mdash;across all surfaces&mdash;is recorded as a triple of <strong>actor</strong> (the student), <strong>verb</strong> (what they did: messaged, received, started, cleared, selected, switched), and <strong>object</strong> (what they acted on: a tutor agent, a conversation, a course tutor, a session). Each activity record carries the full course hierarchy for contextual grounding, optional result data (success, score, duration), and a flexible extensions field for surface-specific metadata.</p>

<p>This unified layer enables cross-surface analysis. A researcher can trace a student's journey from starting a tutoring session, to selecting an agent, to exchanging messages, to switching agents, to clearing a conversation and starting over&mdash;all as a coherent sequence of activities regardless of which surface the interactions occurred on.</p>

<h3>9.3 Emotional Pulse</h3>

<p>LAILA includes a lightweight affective feedback mechanism called the Emotional Pulse. Students can self-report their current emotional state by selecting from seven options: productive, stimulated, frustrated, learning, enjoying, bored, or quitting. These reports can be tagged with a context (chatbot interaction, lesson content, or assignment work) and optionally linked to a specific agent or content item. The pulse is designed to be fast and low-friction&mdash;a single click rather than a questionnaire&mdash;so that students can report their emotional state in the moment without disrupting their work.</p>

<p>The seven emotions were chosen to capture the affective states most relevant to learning engagement. "Productive" and "stimulated" indicate positive cognitive engagement. "Learning" and "enjoying" indicate positive educational outcomes. "Frustrated" and "bored" are warning signals that the student may need a different type of support. "Quitting" is an alarm&mdash;it indicates the student is about to disengage entirely, and when correlated with agent interaction data, it can reveal which agents or interaction patterns are associated with dropout.</p>

<h3>9.4 Cross-Surface Message Tracing</h3>

<p>Every AI interaction in the system, regardless of surface, writes a detailed per-message record that includes a module identifier indicating which surface generated it. Global tutoring messages are tagged with the agent's name, collaborative messages with the agent and collaboration prefix, course tutor messages with the chatbot name, forum AI posts with a forum identifier, lecture section chatbots with the section identifier, and lecture AI helper interactions with the lecture identifier. This tagging system enables researchers to query all interactions involving a specific agent across every surface, or all interactions on a specific surface across all agents, or all interactions by a specific student across the entire platform.</p>

<h3>9.5 Client-Side Behavioral Analytics</h3>

<p>The frontend complements server-side logging with client-side behavioral analytics. Events are batched (flushed every thirty seconds or at fifty events) and include device detection (browser, operating system, screen dimensions, language, timezone), scroll depth tracking at 25%, 50%, 75%, and 100% thresholds for measuring reading engagement, and session management for grouping events into coherent user sessions. A beacon-based flush on page unload ensures that events are not lost when students navigate away. All analytics data can be exported in CSV, JSON, or Excel format with the full course hierarchy preserved for each record.</p>

<!-- ================================================================
     10. DESIGN PROCESS ANALYTICS
     ================================================================ -->
<h2 id="sec-metrics"><span class="section-number">10.</span> Design Process Analytics</h2>

<p>The agent builder is not just an assignment tool; it is a research instrument. Every aspect of the design process is instrumented to produce data about how students think about tutoring, how they iterate on designs, and how their understanding evolves over time.</p>

<p>At the coarsest level, each student's agent configuration tracks four summary metrics: <strong>total design time</strong> (cumulative seconds spent in the builder), <strong>test conversation count</strong> (how many times the student tested their agent), <strong>iteration count</strong> (how many times they saved changes after testing), and a <strong>version number</strong> that increments with each save. These metrics provide a high-level picture of design engagement&mdash;a student with many iterations and test conversations likely engaged more deeply than one who configured the agent once and submitted.</p>

<p>At a finer level, the system maintains a <strong>configuration change audit trail</strong>. Every time a student modifies their agent's configuration, the system records which fields changed, what the previous values were, and what the new values are, as full before-and-after snapshots. This trail makes it possible to reconstruct the complete evolution of an agent from its initial template selection through every subsequent modification to its final submitted form. A researcher can observe, for example, that a student started with the Socratic Guide template, changed the personality from "socratic" to "friendly" after testing, added three new don't rules, rewrote the system prompt twice, and increased the temperature from 0.7 to 0.8 before submitting&mdash;and can examine each intermediate state to understand the reasoning behind each change.</p>

<p>At the finest level, a <strong>design event log</strong> captures individual interaction events within the builder: which tab the student navigated to, how long they spent on each tab, which field they edited, whether the change was typed or pasted, which role template they selected, which personality preset they applied, when they started and ended test conversations, and the version of the agent at each event. Events are grouped into design sessions, enabling session-level analysis of design patterns.</p>

<p>Test conversations themselves are logged with the full agent configuration snapshot at the time of testing, creating a paired record of "what was the agent configured to do?" and "what did it actually do in conversation?" This pairing is essential for understanding how students evaluate and refine their designs: by comparing the configuration snapshot to the conversation content, researchers (and instructors) can see whether the agent behaved as the student intended, and how the student responded to gaps between intention and behavior.</p>

<p>Instructors review submitted agents through a dedicated grading interface that surfaces all of this data: the final configuration, the test conversation history (with version tags), the change trail, and the student's reflection responses. This creates a uniquely rich assessment artifact&mdash;not a static document, but a record of an iterative design process that reveals the student's thinking about pedagogy, their capacity for self-evaluation, and their willingness to revise based on evidence.</p>

<!-- ================================================================
     11. AI-POWERED CONTENT GENERATION
     ================================================================ -->
<h2 id="sec-generation"><span class="section-number">11.</span> AI-Powered Content Generation</h2>

<p>Beyond tutoring conversations, LAILA uses the same multi-provider LLM infrastructure to generate educational content on demand. Two generation services&mdash;one for multiple-choice questions, one for surveys&mdash;allow instructors and students to produce assessment and feedback instruments in seconds rather than hours. Both services follow a shared architectural pattern: a configurable system prompt defines the AI's role as an assessment designer, format instructions specify the expected JSON output structure, and a validation pipeline catches and corrects malformed responses before they reach the user. All generation prompts, format instructions, and defaults are stored in the database as system settings, making them adjustable by administrators without code changes.</p>

<h3 id="sec-generation-mcq">11.1 MCQ Generation for Quizzes</h3>

<p>The MCQ generation service produces multiple-choice questions from a topic description and optional source content. Instructors specify the number of questions (one to ten), difficulty level (easy, medium, or hard), and the number of options per question (three to five). The AI is instructed to act as an "expert educational assessment designer" and follows explicit guidelines that shape the pedagogical quality of the output: test comprehension and application rather than mere recall, ensure all distractors are plausible but clearly wrong, avoid "all of the above" or "none of the above" options, and match the difficulty level to Bloom's taxonomy&mdash;easy questions test basic understanding, medium questions test application, and hard questions test analysis, synthesis, and evaluation.</p>

<p>The service operates at a low temperature of 0.4, reflecting the need for precision and consistency in assessment items. When source content is provided, the system truncates it to 8,000 characters and instructs the AI to base questions on the key concepts from that content, grounding the generated questions in specific material rather than generic knowledge. The AI's response is parsed through a multi-stage extraction pipeline that handles markdown code fences, strips reasoning traces from models that produce them, and falls back to regex-based JSON extraction when the response contains extraneous text.</p>

<p>A validation step then checks each generated question against structural requirements: the question text, options array, and correct answer must all be present; the correct answer must exactly match one of the options (with case-insensitive comparison); and letter references like "A" or "Option B" are automatically resolved to the corresponding option text. Questions that fail validation are logged and silently dropped, and the response metadata reports both the requested and actual question counts so the caller knows if any were lost.</p>

<h3 id="sec-generation-practice">11.2 Practice Question Generation</h3>

<p>A distinct endpoint allows <em>students</em> to generate practice questions from lecture content for self-study. Given a lecture identifier, the service retrieves the lecture's text sections and AI-generated content sections, concatenates them into a single content block, and feeds this to the MCQ generation pipeline with the course title as additional context. Access control ensures that only enrolled students, course instructors, and administrators can generate practice questions for a given lecture, and the system returns the questions directly to the student without persisting them as a formal quiz&mdash;they are ephemeral study aids, not graded assessments.</p>

<p>This feature transforms every lecture in the platform into a potential source of self-assessment material. A student finishing a lecture on sorting algorithms can immediately generate five medium-difficulty practice questions based on the exact content they just read, test their understanding, review the AI-generated explanations for any questions they got wrong, and generate a new set if they want more practice. The pedagogical value lies in the immediacy and specificity: the questions are grounded in the actual lecture content, not a generic question bank.</p>

<h3 id="sec-generation-survey">11.3 Survey Generation</h3>

<p>The survey generation service follows the same architectural pattern as MCQ generation but produces a different kind of instrument. Instructors specify a topic, question count (one to fifteen), and survey type, and the service generates a complete survey with title, description, and an ordered set of questions ready for deployment.</p>

<p>Five survey types are supported, each with a distinct pedagogical prompt. <strong>General feedback</strong> produces a mix of question types covering overall satisfaction, specific aspects, and open-ended feedback opportunities. <strong>Course evaluation</strong> focuses on teaching quality, content relevance, workload, engagement, and areas for improvement, using mostly Likert-scale questions with a few open-ended items. <strong>Likert scale</strong> generates exclusively single-choice questions with a standardized five-point agreement scale (Strongly Disagree through Strongly Agree). <strong>Learning strategies</strong> produces questions about study habits, metacognition, self-regulation, time management, and collaborative learning. And <strong>custom</strong> defers entirely to the instructor's additional instructions, allowing free-form specification of what the survey should cover.</p>

<div class="diagram">
  <div class="diagram-title">Figure 3 &mdash; AI Content Generation Pipeline</div>
  <div class="flow-row">
    <div class="flow-node node-primary">Input</div>
    <span class="flow-arrow">&rarr;</span>
    <div class="flow-node node-secondary">Prompt Assembly</div>
    <span class="flow-arrow">&rarr;</span>
    <div class="flow-node node-warn">LLM Provider</div>
    <span class="flow-arrow">&rarr;</span>
    <div class="flow-node node-secondary">JSON Extraction</div>
    <span class="flow-arrow">&rarr;</span>
    <div class="flow-node node-secondary">Validation</div>
    <span class="flow-arrow">&rarr;</span>
    <div class="flow-node node-highlight">Output</div>
  </div>
</div>

<p>The generated questions support three types: <code>single_choice</code> for questions with one correct or preferred answer, <code>multiple_choice</code> for questions where multiple options may apply, and <code>free_text</code> for open-ended responses. The validation pipeline normalizes common type variations&mdash;mapping "radio" to single_choice, "checkbox" to multiple_choice, "open_ended" to free_text&mdash;and ensures that choice questions have at least two options while free-text questions have none. A transactional database operation then creates the survey and all its questions atomically, so the instructor receives a fully formed survey ready for review and publication.</p>

<!-- ================================================================
     12. THE ASSESSMENT ENGINE
     ================================================================ -->
<h2 id="sec-assessment"><span class="section-number">12.</span> The Assessment Engine</h2>

<p>LAILA includes a comprehensive quiz and assessment system that operates alongside the tutoring agents, providing structured evaluation capabilities that complement the agents' conversational support. Where agents help students learn through dialogue, the assessment engine measures what they have learned through formal, graded instruments.</p>

<h3 id="sec-assessment-architecture">12.1 Quiz Architecture</h3>

<p>Each quiz belongs to a course module and carries a rich set of configuration options that control the assessment experience. Instructors set <strong>time limits</strong> (in minutes), <strong>maximum attempts</strong> per student, a <strong>passing score</strong> threshold (as a percentage), and an <strong>availability window</strong> defined by start and due dates. Two shuffle options&mdash;one for questions, one for answer options&mdash;allow instructors to randomize the assessment for academic integrity while maintaining the same content across students. A <strong>result display</strong> setting controls when students can see their scores and correct answers: immediately after submission, only after the due date passes, or never.</p>

<p>Quizzes are created either manually, with instructors adding questions one at a time through a form interface, or through <strong>bulk generation</strong> using the AI-powered MCQ service. The bulk pathway generates questions from a topic and difficulty level, then inserts them into the quiz in a single operation. Instructors can reorder questions after creation, maintaining custom sequencing that persists across student views (unless shuffling is enabled).</p>

<h3 id="sec-assessment-types">12.2 Question Types and Grading</h3>

<p>The quiz system supports four question types, each with distinct grading logic. <strong>Multiple choice</strong> questions present three to five options with exactly one correct answer; grading is binary (full points or zero). <strong>True/false</strong> questions are a constrained variant of multiple choice with exactly two options. <strong>Short answer</strong> questions require the student to type a response; these are flagged for manual review by the instructor, as automated grading of free-text responses requires nuanced evaluation. <strong>Fill-in-the-blank</strong> questions present a statement with a missing element; grading uses exact string matching against the expected answer.</p>

<p>Each question carries a configurable point value, and the quiz's total score is the sum of points across all questions. Auto-grading runs immediately for objective question types (multiple choice, true/false, fill-in-the-blank), calculating the student's score as a percentage and comparing it against the passing threshold. Short answer questions remain unscored until the instructor reviews them, at which point the total score is recalculated.</p>

<h3 id="sec-assessment-student">12.3 Student Experience</h3>

<p>When a student starts a quiz attempt, the system records the start time and, if shuffling is enabled, generates a randomized ordering that persists for that specific attempt. Students can <strong>auto-save</strong> individual answers as they work, ensuring that progress is not lost if they navigate away or lose connectivity. If a student leaves and returns before the time limit expires, they can <strong>resume</strong> their in-progress attempt with all previously saved answers intact.</p>

<p>Upon submission, the system calculates the elapsed time, runs auto-grading, and persists the attempt with its score, pass/fail status, and per-question answer records including correctness flags and points awarded. Students who have not exhausted their maximum attempts can start a new attempt, and the system tracks all attempts independently, allowing instructors to see the full history of a student's engagement with the assessment. Cross-course quiz browsing lets students discover available quizzes across all their enrolled courses from a single interface.</p>

<!-- ================================================================
     13. CUSTOM LABS & CODE EXECUTION
     ================================================================ -->
<h2 id="sec-labs"><span class="section-number">13.</span> Custom Labs and Code Execution</h2>

<p>LAILA extends the learning environment beyond text and dialogue into hands-on computational practice through a custom lab system. Labs provide structured, template-based coding exercises that run in the browser, enabling students to write and execute R code without installing any software locally. The lab system bridges the gap between conceptual understanding (supported by the tutoring agents) and practical application (supported by code execution), creating a complete learning cycle within a single platform.</p>

<h3 id="sec-labs-types">13.1 Lab Types and Templates</h3>

<p>The system supports ten lab types, each representing a distinct analytical methodology: <strong>Transition Network Analysis</strong> (TNA) using the R <code>tna</code> package, <strong>Statistics</strong> for general data analysis, <strong>Network Analysis</strong>, <strong>Sequence Analysis</strong>, <strong>Data Visualization</strong> with <code>ggplot2</code>, <strong>Regression Analysis</strong>, <strong>Clustering</strong>, <strong>Time Series</strong>, <strong>Text Analysis</strong>, and a <strong>Custom</strong> type for free-form R labs. Each lab type can carry a set of pre-built code templates that serve as starting points for student work.</p>

<p>The pre-built templates are pedagogically substantial. The Statistics lab ships with eleven templates covering descriptive statistics, independent and paired t-tests with effect sizes, correlation analysis with p-values, simple and multiple regression with confidence intervals, one-way and two-way ANOVA with post-hoc tests, chi-square tests of independence with Cram&eacute;r's V, and normality testing with skewness and kurtosis. The TNA lab provides five templates that walk students through loading data, creating network models, computing centralities, and visualizing transition networks. Three survey-instrument labs provide simulated data and analysis templates for established educational questionnaires: the <strong>MSLQ</strong> (Motivated Strategies for Learning Questionnaire; Pintrich et al., 1991) with motivation subscales predicting academic achievement, the <strong>COLLES</strong> (Constructivist On-Line Learning Environment Survey; Taylor &amp; Maor, 2000) measuring six dimensions of online learning quality, and the <strong>R-SPQ-2F</strong> (Revised Study Process Questionnaire; Biggs, Kember &amp; Leung, 2001) comparing deep and surface learning approaches.</p>

<table>
  <thead>
    <tr>
      <th>Lab Type</th>
      <th>Templates</th>
      <th>Focus Area</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>TNA</td><td>5</td><td>Transition network models, centralities, visualization</td></tr>
    <tr><td>Statistics</td><td>11</td><td>t-tests, ANOVA, regression, correlation, normality</td></tr>
    <tr><td>MSLQ Survey</td><td>3</td><td>Motivation &amp; self-regulated learning (Pintrich, 1991)</td></tr>
    <tr><td>COLLES Survey</td><td>3</td><td>Online learning environment quality (Taylor &amp; Maor, 2000)</td></tr>
    <tr><td>R-SPQ-2F Survey</td><td>3</td><td>Deep vs. surface learning approaches (Biggs et al., 2001)</td></tr>
    <tr><td>Custom</td><td>User-defined</td><td>Free-form R code exercises</td></tr>
  </tbody>
</table>

<p>Each template includes a title, description, and starter code that students can modify and execute. Templates are ordered within a lab, creating a guided progression from simpler to more complex analyses. Instructors can add, edit, delete, and reorder templates to customize the learning sequence for their specific pedagogical goals.</p>

<h3 id="sec-labs-execution">13.2 Code Execution Environment</h3>

<p>Code execution takes place in the browser through WebR, a WebAssembly-based R interpreter that runs entirely client-side. The execution environment is built around a Monaco editor component&mdash;the same editor that powers Visual Studio Code&mdash;providing syntax highlighting, auto-completion, and a professional coding experience. Each code block within a lab is independently executable: students can run, reset, and request AI help on individual blocks without affecting others.</p>

<p>The execution pipeline captures four types of output: <strong>stdout</strong> (standard printed output), <strong>stderr</strong> (error messages and warnings), <strong>plot</strong> (graphical output from R's plotting functions), and <strong>message</strong> (informational messages from R packages). Outputs are displayed below the editor in a structured format that distinguishes normal output from errors. When execution fails, a help button allows students to send the code and error message to one of the tutoring agents for explanation&mdash;connecting the lab environment back to the multi-agent system.</p>

<p>Code persistence within a session ensures that students' modifications are preserved as they work through a lab's templates. Execution results are reported to the parent component, enabling integration with the learning analytics system: the platform can track which blocks each student ran, what code they wrote, whether execution succeeded, and what output was produced.</p>

<h3 id="sec-labs-deployment">13.3 Lab Assignment and Deployment</h3>

<p>Labs follow the same deployment model as the tutoring agents: a lab is defined once and then assigned to courses. Instructors assign labs to specific courses and optionally to specific modules within courses, creating a contextual relationship between the lab and the course content. Access control follows a layered model: administrators can access all labs, creators can access their own labs, public labs are available to everyone, and private labs are accessible only to students enrolled in courses where the lab has been assigned (or to instructors of those courses).</p>

<p>The assignment system supports a many-to-many relationship: a single lab can be assigned to multiple courses, and a single course can have multiple labs assigned. Instructors can filter available labs by type, search by name or description, and view their own labs separately from public ones. This architecture encourages a shared library of lab exercises that can be reused and adapted across courses and semesters.</p>

<!-- ================================================================
     14. MULTI-PROVIDER LLM INFRASTRUCTURE
     ================================================================ -->
<h2 id="sec-llm"><span class="section-number">14.</span> Multi-Provider LLM Infrastructure</h2>

<p>Every AI interaction in LAILA&mdash;tutoring conversations, content generation, routing decisions, forum participation&mdash;passes through a unified LLM service layer that abstracts the differences between providers into a single interface. This abstraction is not merely a convenience; it is an architectural commitment to provider independence, ensuring that the pedagogical logic of the system is never coupled to a specific vendor's API.</p>

<h3 id="sec-llm-abstraction">14.1 Provider Abstraction Layer</h3>

<p>The LLM service presents a single <code>chat()</code> method that accepts a provider-agnostic request containing messages, temperature, token limits, and sampling parameters. Internally, it resolves the target provider (either explicitly specified or falling back to the configured default), looks up the provider's connection details and capabilities, and dispatches the request through a provider-specific adapter. The response is normalized into a common format containing the generated text, usage statistics (prompt and completion tokens), and the response time&mdash;regardless of whether the underlying call went to OpenAI, Anthropic, Google, or a local model.</p>

<p>Provider selection follows a priority chain. If the request specifies a provider by name, that provider is used. Otherwise, the system uses the provider marked as the default. If no default is set, it falls back to the highest-priority enabled provider. This chain ensures that the system always has a viable provider as long as at least one is configured and enabled.</p>

<div class="diagram">
  <div class="diagram-title">Figure 4 &mdash; Multi-Provider LLM Architecture</div>
  <div class="tier-diagram">
    <div class="tier-label">Pedagogical Services</div>
    <div class="tier">
      <div class="node node-primary">Tutoring Chat</div>
      <div class="node node-primary">MCQ Generation</div>
      <div class="node node-primary">Survey Generation</div>
      <div class="node node-primary">AI Routing</div>
    </div>
    <div class="arrow-down">&darr;</div>
    <div class="tier-label">Unified LLM Service</div>
    <div class="tier">
      <div class="node node-secondary" style="min-width: 480px;">chat() &mdash; Provider Resolution &mdash; Parameter Merging &mdash; Response Normalization</div>
    </div>
    <div class="arrow-down">&darr;</div>
    <div class="tier-label">Provider Adapters</div>
    <div class="tier">
      <div class="node node-warn">OpenAI</div>
      <div class="node node-warn">Anthropic</div>
      <div class="node node-warn">Gemini</div>
      <div class="node node-warn">Ollama</div>
      <div class="node node-warn">LM Studio</div>
      <div class="node node-warn">Groq</div>
    </div>
  </div>
</div>

<h3 id="sec-llm-providers">14.2 Supported Providers</h3>

<p>The system includes dedicated adapters for four provider families. <strong>OpenAI-compatible providers</strong>&mdash;including OpenAI itself, Azure OpenAI, OpenRouter, Together, Groq, Mistral, and LM Studio&mdash;share a common adapter built on the OpenAI SDK, with special handling for reasoning models (the o1 and o3 series) that do not support temperature or system messages. <strong>Google Gemini</strong> uses the Google Generative AI SDK with safety settings configured to avoid content blocking in educational contexts. <strong>Anthropic</strong> uses a direct HTTP integration with the Messages API, properly separating system prompts from conversation messages as required by Anthropic's format. <strong>Ollama</strong> uses a native HTTP integration for local model inference, with additional management endpoints for listing and pulling models.</p>

<p>Each provider is configured with over fifty parameters covering connection details (base URL, API key, organization ID), generation defaults (temperature, max tokens, top-p, top-k, frequency and presence penalties), timeout and retry settings (request timeout, connect timeout, maximum retries with exponential backoff), rate limiting (requests per minute, tokens per minute, requests per day, concurrency limits), capability flags (streaming, vision, function calling, JSON mode, system message support), and network configuration (proxy URL, custom headers, TLS verification). These parameters are persisted in the database and editable through an administrative interface, allowing deployment-time configuration without code changes.</p>

<h3 id="sec-llm-config">14.3 Configuration and Resilience</h3>

<p>The service includes a health-checking system that periodically tests each provider's connectivity and responsiveness. Health checks use the lightest possible probe for each provider type: model listing for OpenAI-compatible providers, a minimal generation request for Gemini, the tags endpoint for Ollama, and a five-token completion for Anthropic. The results&mdash;healthy, unhealthy, or unknown&mdash;are recorded along with latency measurements, consecutive failure counts, and the last error message, providing operators with visibility into provider reliability.</p>

<p>Usage statistics are tracked per provider: total requests, total tokens consumed, and total errors. These counters enable cost monitoring and capacity planning. A provider cache with a five-minute expiry reduces database lookups for hot-path operations, and the cache is automatically invalidated when provider configurations change.</p>

<p>The system also supports <strong>local-first deployment</strong> through Ollama and LM Studio integration. Institutions concerned about data privacy can route all AI interactions through locally hosted models, keeping student conversations entirely within their infrastructure. The Ollama adapter includes model management capabilities&mdash;listing available models and pulling new ones&mdash;making it possible to manage the local model library from within LAILA's administrative interface.</p>

<!-- ================================================================
     15. CONTENT SAFETY
     ================================================================ -->
<h2 id="sec-safety"><span class="section-number">15.</span> Content Safety and Integrity</h2>

<p>Because LAILA places AI-generated content in contexts where students read, discuss, and learn from it, content safety is a system-wide concern rather than a per-feature afterthought. The platform implements safety measures at multiple layers.</p>

<p>At the <strong>output layer</strong>, all AI-generated content&mdash;whether from tutoring conversations, forum posts, or generated questions&mdash;passes through HTML sanitization before rendering. The system uses DOMPurify to strip potentially dangerous elements while preserving safe formatting. A custom markdown renderer converts AI responses into structured HTML with Tailwind-styled headers, code blocks with language hints, and properly attributed links that open in new tabs with <code>rel="noopener noreferrer"</code> to prevent tab-napping attacks. This dual-pass approach (markdown rendering followed by HTML sanitization) ensures that AI-generated content is both well-formatted and safe.</p>

<p>At the <strong>prompt layer</strong>, the agent system's four-layer prompt construction includes explicit behavioral constraints that function as safety guardrails. Every agent is instructed to stay in character, avoid harmful content, and respect the educational context. The do's and don'ts system provides fine-grained control over what agents can and cannot say&mdash;constraints that are enforced by the prompt rather than by post-processing, which means they shape the generation itself rather than filtering it after the fact.</p>

<p>At the <strong>infrastructure layer</strong>, the multi-provider architecture supports Gemini's built-in safety settings (configurable per harm category with adjustable thresholds), Anthropic's Constitutional AI guardrails, and OpenAI's content policy enforcement. Rate limiting is applied per provider to prevent abuse, and all API keys are stored in the database with the expectation of encryption at rest. The system also supports proxy routing and custom TLS certificates for institutions that require traffic inspection or that operate behind restrictive firewalls.</p>

<p>At the <strong>analytics layer</strong>, the comprehensive logging described in Section 9 provides a complete audit trail of every AI interaction. If a safety concern is identified&mdash;whether through student reports, instructor review, or automated monitoring&mdash;the logs make it possible to trace the exact prompt, response, agent configuration, and context that produced the concerning content, enabling rapid diagnosis and remediation.</p>

<!-- Footer -->
<div style="margin-top: 4rem; padding-top: 1.5rem; border-top: 2px solid var(--accent-light); font-size: 0.85rem; color: var(--text-secondary); text-align: center;">
  <p>LAILA Multi-Agent Tutoring System &mdash; Technical Report &mdash; February 2026</p>
</div>

</body>
</html>
